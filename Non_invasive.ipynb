{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb380ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wfdb\n",
    "import scipy.signal as signal\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac5c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read .dat and .hea files and return the ECG signals\n",
    "def read_ecg_files(file_path):\n",
    "    record = wfdb.rdrecord(file_path)\n",
    "    ecg_signals = record.p_signal\n",
    "    return ecg_signals\n",
    "\n",
    "# List all files in the database\n",
    "arr_files = [f'ARR_{i:02d}' for i in range(1, 13)]\n",
    "nrr_files = [f'NR_{i:02d}' for i in range(1, 15)]\n",
    "\n",
    "# Read all files and store signals\n",
    "arr_signals = [read_ecg_files(file) for file in arr_files]\n",
    "nrr_signals = [read_ecg_files(file) for file in nrr_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73bee368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Least Mean Square (NLMS) adaptive filter\n",
    "def nlms_filter(primary_signal, reference_signal, step_size=0.1, num_coeffs=15):\n",
    "    num_samples = primary_signal.shape[0]\n",
    "    output_signal = np.zeros_like(primary_signal)\n",
    "    weights = np.zeros((num_coeffs, primary_signal.shape[1]))\n",
    "\n",
    "    # Padding the primary_signal to handle the initial window\n",
    "    padded_signal = np.pad(primary_signal, ((num_coeffs - 1, 0), (0, 0)), mode='constant')\n",
    "\n",
    "    for n in range(num_samples):\n",
    "        for ch in range(primary_signal.shape[1]):\n",
    "            x = padded_signal[n:num_coeffs + n, ch][::-1]\n",
    "            y = np.dot(weights[:, ch], x)\n",
    "            e = reference_signal[n, ch] - y\n",
    "            mu = step_size / (np.dot(x, x) + 1e-6)  # Update step size\n",
    "            weights[:, ch] += mu * e * x\n",
    "            output_signal[n, ch] = y\n",
    "\n",
    "    return output_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6310a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NLMS filter in parallel\n",
    "def parallel_nlms_filter(signals):\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        filtered_signals = pool.starmap(nlms_filter, [(sig, sig) for sig in signals])\n",
    "    return filtered_signals\n",
    "\n",
    "filtered_arr_signals = parallel_nlms_filter(arr_signals)\n",
    "filtered_nrr_signals = parallel_nlms_filter(nrr_signals)\n",
    "# One-dimensional convolution with Gaussian wavelet\n",
    "def gaussian_wavelet(size, sigma):\n",
    "    x = np.linspace(-size // 2, size // 2, size)\n",
    "    wavelet = np.exp(-0.5 * (x / sigma) ** 2)\n",
    "    return wavelet / np.sum(wavelet)\n",
    "\n",
    "wavelet = gaussian_wavelet(size=15, sigma=2)\n",
    "\n",
    "def apply_wavelet_convolution(signal, wavelet):\n",
    "    convoluted_signal = np.zeros_like(signal)\n",
    "    for ch in range(signal.shape[1]):\n",
    "        convoluted_signal[:, ch] = convolve(signal[:, ch], wavelet, mode='same')\n",
    "    return convoluted_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b20739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom implementation of approximate entropy\n",
    "def approximate_entropy(U, m, r):\n",
    "    def _maxdist(x_i, x_j):\n",
    "        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "    \n",
    "    N = len(U)\n",
    "    x = [[U[j] for j in range(i, i + m)] for i in range(N - m + 1)]\n",
    "    C = []\n",
    "    for x_i in x:\n",
    "        C.append(len([1 for x_j in x if _maxdist(x_i, x_j) <= r]) / (N - m + 1))\n",
    "    C = np.log(C)\n",
    "    return sum(C) / (N - m + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56356846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature extraction function\n",
    "def extract_features(signal):\n",
    "    features = []\n",
    "    for ch in range(signal.shape[1]):\n",
    "        ch_signal = signal[:, ch]\n",
    "        mean = np.mean(ch_signal)\n",
    "        std_dev = np.std(ch_signal)\n",
    "        rms = np.sqrt(np.mean(ch_signal**2))\n",
    "        skewness = skew(ch_signal)\n",
    "        kurt = kurtosis(ch_signal)\n",
    "        zero_crossings = ((ch_signal[:-1] * ch_signal[1:]) < 0).sum()\n",
    "        entropy = approximate_entropy(ch_signal, m=2, r=0.2 * np.std(ch_signal))\n",
    "        \n",
    "        # Here we assume autoregressive coefficients and Higuchiâ€™s fractal dimension are provided by custom functions\n",
    "        ar_coeffs = compute_ar_coeffs(ch_signal, order=4)\n",
    "        higuchi_fd = compute_higuchi_fd(ch_signal, kmax=10)\n",
    "        \n",
    "        features.extend([mean, std_dev, rms, skewness, kurt, zero_crossings, entropy, higuchi_fd, *ar_coeffs])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def compute_ar_coeffs(signal, order):\n",
    "    from statsmodels.tsa.ar_model import AutoReg\n",
    "    model = AutoReg(signal, lags=order)\n",
    "    model_fit = model.fit()\n",
    "    return model_fit.params[1:]\n",
    "\n",
    "def compute_higuchi_fd(signal, kmax):\n",
    "    # Placeholder for Higuchi's fractal dimension calculation\n",
    "    # Implement Higuchi's algorithm or use an appropriate library function\n",
    "    return np.random.random()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b3330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from all convoluted signals\n",
    "arr_features = np.array([extract_features(sig) for sig in convoluted_arr_signals])\n",
    "nrr_features = np.array([extract_features(sig) for sig in convoluted_nrr_signals])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8506ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume labels are 0 for NRR and 1 for ARR\n",
    "labels_arr = np.ones(len(arr_features))\n",
    "labels_nrr = np.zeros(len(nrr_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e13545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and labels\n",
    "features = np.vstack((arr_features, nrr_features))\n",
    "labels = np.hstack((labels_arr, labels_nrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f897afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Build the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.05), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
